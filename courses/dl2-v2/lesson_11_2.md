# Lesson 11_2:  Neural Translation

(09-Apr-2018, live)  

- [Wiki Lesson 11](http://forums.fast.ai/t/part-2-lesson-11-wiki/14699)
- [Video Lesson 11](https://www.youtube.com/watch?v=tY0n9OT5_nA&feature=youtu.be) 
  - video length:  2:15:57
- http://course.fast.ai/lessons/lesson11.html
- Notebook:  
   * [translate.ipynb](https://github.com/fastai/fastai/blob/master/courses/dl2/translate.ipynb)
   
## `01:13:10` After Break
- So one question that came up during the break is that some of the tokens that are missing in fast text like had a curly quote rather than a straight quote, for example.  And the question was, would it help to normalize punctuation?  And, the answer for this particular case is, probably yes, the difference between curly quotes and straight quotes is really semantic.  You do have to be very careful, though, because like it may turn out that people using beautiful curly quotes are like using more formal language and they're actually writing in a different way so I generally... you know, if you're going to do some kind of pre-processing like punctuation normalization, you should definitely check your results with and without because like *nearly always* that kind of pre-processing makes things worse even when I'm sure it won't.

### `01:14:10`  Question
- Person x (Yannet?):  Hello, what might be some ways of realizing? these sequence to sequence models besides dropout and weight decay?
- JH:  Let me think about that during the week. Yeah, it's like you know, AWD LSTM, which we've been relying on a lot, has so many great.. I mean it's all dropout, well not all dropout,  there's dropout of many different kinds. And then there's the... we haven't talked about it very much, but there's also a kind of regularization based on activations and stuff like that as well.  And on changes, and whatever.  I just haven't seen anybody put anything like that amount of work into regularization of sequence to sequence models and I think there's a huge opportunity for somebody to do like the AWD LSTM of seq to seq, which might be as simple as dealing with all the ideas from AWD LSTM and using them directly in seq to seq.  That would be pretty easy to try, I think.  

#### Steven Merity paper
- And there's been an interesting paper that actually Steven Merity's added in the last couple of weeks where he used an idea which I don't know if he stole it from me but it was certainly something I had also recently done and talked about on Twitter.  Either way, I am thrilled that he's done it which was to take all of those different AWD LSTM hyperparameters and train a bunch of different models and then use a random forest to find out with feature importance, which ones actually matter the most, and then figure out how to set them. Yes, so I think you could totally, you know, use this approach, to figure out, you know, for sequence to sequence regularization approaches, which one is the best and optimize them.  And that would be amazing. Yeah, but at the moment, I think, you know, I don't know that there are additional ideas to sequence to sequence regularization that I can think of beyond what's in that paper for regular language model stuff and probably all those same approaches would work.

## `01:16:30` Bi-directional
- Okay, so tricks.  Trick #1, go **bi-directional**.  So, for classification, my approach to bi-directional that I suggested you use is take all of your token sequences, spin them around and train a new language model and train a new classifier. And I also mentioned the wiki text pre-trained model if you replace FWD with VWD in the name, you'll get the pre-trained backward model I created for you, okay, so you can use that.  Get a set of predictions and then average the predictions just like a normal ensemble, okay.  And that's kind of how we do bi-dira [bi-directional] for that classification.  There may be ways to do it, end-to-end, but I haven't quite figured them out yet.  They're not in fastai yet, and I don't think anybody's written a paper about them yet, so if you figure it out, that's an interesting line of research.  But, because we're not doing, you know, massive documents where we have kind of to chunk it into separate bits and then pool over them and whatever, we can do bi-dir very easily, in this case.  Which is literally as simple as adding `bidirectional=True` to our encoder (`self.gru_enc`).  People tend not to do bi-directional for the decoder.  I think, partly because it's kind of considered cheating, but I don't know.  Like I was just talking to somebody at the break about it.  Maybe it can work in some situations.  Although it might need to be more of an ensembling approach in the decoder because you kind of, it's a bit less obvious.  Anyway, in the encoder, it's very, very simple:  `bidirectional=True` and we now have... With `bidirectional=True`, rather than just having an RNN which is going this direction [right], we have a second RNN that's going in this direction [left].  And so that second RNN literally is visiting them each token in the opposing order.  So, when we get the final hidden state, it's here [left], rather than here [right].  But, the hidden state is of the same size, so the final result is that we end up with a tensor that's got an extra too-long axis, right.  And, depending on what library you use, often that will then be combined with the number of layers things.  So, if you've got 2 layers, and bi-directional, that tensor dimensions is now length 4.  With PyTorch, it kind of depends which bit of the process you're looking at as to whether you get a separate result for each layer and offer each bidirectional bit, and so forth.  You have to look up the docs and it will tell you inputs/outputs, tensor sizes appropriate for the number of layers and whether you have `bidirectional=True`.
- In this particular case, you'll basically see all the changes I've had to make.  So, for example, you'll see when I added bidirectional=True, my linear layer now needs `nh*2` (number of hidden times 2) to reflect the fact that we have that second direction in our hidden state now.  You'll see in `def initHidden`, it's now  `self.nl*2` now here, okay. So, you'll see there's a few places where there's been an extra two that has to be thrown in.  Yes, Yannet?
- [Yannet Interian](https://www.linkedin.com/in/interian/): Why making a decoder by the original is considered cheating?  
- JH:  Well, it's not just cheating, it's like we have this loop going on, you know.  It's not as simple as just kind of having two tensors.  And, then, like how do you turn those two separate loops into a final result.  You know, after talking about it during the break, I've kind of gone from, like, "hey, everyone knows it doesn't work" to "oh, maybe it kind of could work but it requires more thought.  It's quite possible during the week I realize it's a dumb idea and I was being stupid, but we'll think about it.
- Yannet:  Another question people had, why do you need to have an in? to that loop?
- JH:  Why do I have a what to the loop?
- YI:  Why do you need to, like have a, an end to that loop.  You have like a range.  If your range...
- JH:  range? oh, yeah, I mean it's because when I start training, everything's random.  So, this will probably never be true:  `if (dec_inp==1).all(): break`. So, later on, it'll pretty much always break out eventually.  But, yeah, it's basically like we're going to go for it.  It's really important to remember like when you're designing an architecture that when you start, the model knows nothing about anything.  So you kind of want to make sure it's doing something at least vaguely sensible.
- So, bidirectional means we had, you know, let's see how we got here, we got out to 3.58 cross-entropy loss.  With a single direction.  With bidirection, gets down to 3.51. So that improved it a bit.  That's good and as I say, it's the only... it shouldn't really slow things down too much.  You know, bidirectional does mean there's a little bit more sequential processing have to happen.  But, you know, generally it's a good win.  In the google translation model of the 8 layers, only the first layer is bidirectional because it allows it do more in parallel. So, if you create really deep models, you may need to think about which ones are bidirectional.  Otherwise, we have performance issues. okay, so 3.51 [cross-entropy loss].

## `01:22:35` Teacher Forcing
- Now, let's talk about teacher forcing.
- So, teacher forcing is... I'm going to come back to this idea that when the model starts learning, it knows nothing about nothing. So, when the model starts learning, it is not going to spit out "Er" at this point [Reference:  slide on translating "He loved to eat."], it's going to spit out some random meaningless word. Because it doesn't know anything about German or about English or about the idea of language or anything.  And then it's going to feed it down here as an input and be totally unhelpful.  And so that means that early learning is going to be very, very difficult because it's feeding in an input that's stupid into a model that knows nothing.  And somehow it says, get better, right.  So, that's... it's not asking too much... it eventually gets there, but it's definitely not as helpful as we can be.  So, what if, instead of feeding in, what if instead of feeding in the thing I predicted, just now right, what if instead we feed in the actual correct word it was meant to be?  Now, we can't do that at inference time because by definition we don't know the correct word. It has to translate it.  We can't require the correct translation in order to do translation.  So, the way I've set this up, I've got this thing called `pr_force` which is probability of 
